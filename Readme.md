# Example of a simple CSV crawler

Small example of a multiple-instance CSV crawler based on top of `gRPC` servers (balanced by `HAProxy`) with [`twirp`](https://github.com/twitchtv/twirp) framework for sever generating and with `mongoDB` as a storage. The crawler could fetch data from remote endpoints, parse and then insert/update into database. Additionally, have a functionality for retrieving already save data with sorting/paging (keyset pattern).

## Commands
```
make install // install code-generation tools
make build  // generate code and build app artifact
make start // start docker-compose cluster
make stop // stop docker-compose cluster
```

## Examples
Because server generated by a twirp could accept any tls/non-tls, http1.1/http2, protobuf/json payloads, there are examples with cURL commmand for visiblility

### Fetch data from remote endpoint
```bash
curl -k -v -X "POST" \
    --header "Content-Type: application/json" \
    --data '{"url": "http://mock-server:8080/1.csv"}' \
    http://localhost:8080/twirp/crawler.Crawler/Fetch
```
In this example, CSV-file would extracted from side-car container with prepopulated data (see `test-data` folder)

### List
Default order, first 5 items
```bash
curl -k -v -X "POST" \
    --header "Content-Type: application/json" \
    --data '{"page_size": 5}' \
    http://localhost:8080/twirp/crawler.Crawler/List
}
```

Default order, retrieving next 5 items
```bash
curl -k -v -X "POST" \
    --header "Content-Type: application/json" \
    --data '{"page_size": 5, "page_token": "60730c2f2c0b72c4d312001b"}' \
    http://localhost:8080/twirp/crawler.Crawler/List
```

Order by price in descending order, first 5 items
```bash
curl -k -v -X "POST" \
    --header "Content-Type: application/json" \
    --data '{"page_size": 5, "search_params": {"order_by": 1, "order_method": 1}}' \
    http://localhost:8080/twirp/crawler.Crawler/List
```

Order by price in descending order, next 5 items
```bash
curl -k -v -X "POST" \
    --header "Content-Type: application/json" \
    --data '{"page_size": 5, "search_params": {"order_by": 1, "order_method": 1}, "page_token": "60730c2f2c0b72c4d3120206_98684"}' \
    http://localhost:8080/twirp/crawler.Crawler/List
```

For more info about methods and models see protobuf file in `/rpc/crawler/service.proto`